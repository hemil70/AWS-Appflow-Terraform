## Requirements

No requirements.

## Providers

| Name | Version |
|------|---------|
| <a name="provider_aws"></a> [aws](#provider\_aws) | n/a |

## Modules

No modules.

## Resources

| Name | Type |
|------|------|
| [aws_appflow_flow.appflow](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/appflow_flow) | resource |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_api_version"></a> [api\_version](#input\_api\_version) | API version that the destination connector uses | `string` | `null` | no |
| <a name="input_description"></a> [description](#input\_description) | Description of the flow | `string` | `null` | no |
| <a name="input_destination_connector_profile_name"></a> [destination\_connector\_profile\_name](#input\_destination\_connector\_profile\_name) | Name of the connector profile. This name must be unique for each connector profile in the AWS account | `string` | `null` | no |
| <a name="input_destination_connector_properties"></a> [destination\_connector\_properties](#input\_destination\_connector\_properties) | Properties that are required to query a particular destination connector | <pre>object({<br>    s3 = optional(object({<br>      bucket_name   = string<br>      bucket_prefix = optional(string)<br>      s3_output_format_config = optional(object({<br>        file_type = optional(string, "JSON") # File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET<br>        aggregation_config = object({<br>          aggregation_type = optional(string) # Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile<br>          target_file_size = optional(number) # The desired file size, in MB, for each output file that Amazon AppFlow writes to the flow destination. Integer value.<br>        })<br>        prefix_config = optional(object({<br>          prefix_format    = string       # Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE<br>          prefix_type      = string       # Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME<br>          prefix_hierarchy = list(string) # Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION<br>        }))<br>        preserve_source_data_typing = optional(bool) # Whether the data types from the source system need to be preserved (Only valid for Parquet file type)<br>      }))<br>    }))<br>    custom_connector = optional(object({<br>      error_handling_config = object({<br>        bucket_name                     = optional(string)<br>        bucket_prefix                   = optional(string)<br>        fail_on_first_destination_error = optional(bool)<br>      })<br>      entity_name          = string                 # Entity specified in the custom connector as a destination in the flow.<br>      custom_properties    = optional(map(string))  # Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.<br>      id_field_names       = optional(list(string)) # Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.<br>      write_operation_type = string                 # Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE<br>    }))<br>    redshift = optional(object({<br>      error_handling_config = object({<br>        bucket_name                     = optional(string)<br>        bucket_prefix                   = optional(string)<br>        fail_on_first_destination_error = optional(bool)<br>      })<br>      intermediate_bucket_name = string           # Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.<br>      bucket_prefix            = optional(string) # Object key for the bucket in which Amazon AppFlow places the destination files.<br>      object                   = optional(string) # Object specified in the Amazon Redshift flow destination.<br>    }))<br>    snowflake = optional(object({<br>      error_handling_config = optional(object({<br>        bucket_name                     = optional(string)<br>        bucket_prefix                   = optional(string)<br>        fail_on_first_destination_error = optional(bool)<br>      }))<br>      intermediate_bucket_name = string           # Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Snowflake.<br>      bucket_prefix            = optional(string) # Object key for the bucket in which Amazon AppFlow places the destination files.<br>      object                   = string           # Object specified in the Amazon Snowflake flow destination<br>    }))<br>  })</pre> | <pre>{<br>  "custom_connector": null,<br>  "redshift": null,<br>  "s3": null,<br>  "snowflake": null<br>}</pre> | no |
| <a name="input_destination_connector_type"></a> [destination\_connector\_type](#input\_destination\_connector\_type) | Type of Destination connector, such as Redshift, S3, and so on | `string` | n/a | yes |
| <a name="input_flow_name"></a> [flow\_name](#input\_flow\_name) | Name of the flow | `string` | n/a | yes |
| <a name="input_incremental_pull_config"></a> [incremental\_pull\_config](#input\_incremental\_pull\_config) | Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull | <pre>object({<br>    datetime_type_field_name = string # Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.<br>  })</pre> | `null` | no |
| <a name="input_kms_arn"></a> [kms\_arn](#input\_kms\_arn) | ARN of the KMS key you provide for encryption | `string` | `null` | no |
| <a name="input_metadata_catalog_config"></a> [metadata\_catalog\_config](#input\_metadata\_catalog\_config) | A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog | <pre>object({<br>    database_name = string # The name of an existing Glue database to store the metadata tables that Amazon AppFlow creates<br>    role_arn      = string # The ARN of an IAM role that grants AppFlow the permissions it needs to create Data Catalog tables, databases, and partitions<br>    table_prefix  = string # A naming prefix for each Data Catalog table that Amazon AppFlow creates<br>  })</pre> | `null` | no |
| <a name="input_source_connector_profile_name"></a> [source\_connector\_profile\_name](#input\_source\_connector\_profile\_name) | Name of the connector profile. This name must be unique for each connector profile in the AWS account | `string` | `null` | no |
| <a name="input_source_connector_properties"></a> [source\_connector\_properties](#input\_source\_connector\_properties) | Properties that are required to query a particular source connector | <pre>object({<br>    s3 = optional(object({<br>      bucket_name   = string               # Amazon S3 bucket name where the source files are stored<br>      bucket_prefix = optional(string, "") # Object key for the Amazon S3 bucket in which the source files are stored<br>      s3_input_format_config = object({<br>        s3_input_file_type = optional(string, "JSON") # File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are CSV and JSON<br>      })<br>    }))<br>    veeva = optional(object({<br>      object               = string           # Object specified in the Veeva flow source<br>      document_type        = optional(string) # Document type specified in the Veeva document extract flow<br>      include_all_versions = optional(bool)<br>      include_renditions   = optional(bool)<br>      include_source_files = optional(bool)<br>    }))<br>    sapo_data = optional(object({<br>      object_path = string # Object path specified in the SAPOData flow source<br>    }))<br>    service_now = optional(object({<br>      object = string # Object specified in the flow source<br>    }))<br>    custom_connector = optional(object({<br>      entity_name       = string                # Entity specified in the custom connector as a source in the flow<br>      custom_properties = optional(map(string)) # Custom properties that are specific to the connector when it's used as a source in the flow. Maximum of 50 items.<br>    }))<br>  })</pre> | <pre>{<br>  "custom_connector": {<br>    "custom_properties": {},<br>    "entity_name": null<br>  },<br>  "s3": {<br>    "bucket_name": null,<br>    "bucket_prefix": "",<br>    "s3_input_format_config": {<br>      "s3_input_file_type": "JSON"<br>    }<br>  },<br>  "sapo_data": {<br>    "object_path": null<br>  },<br>  "service_now": {<br>    "object": null<br>  },<br>  "veeva": {<br>    "document_type": "",<br>    "include_all_versions": false,<br>    "include_renditions": false,<br>    "include_source_files": false,<br>    "object": null<br>  }<br>}</pre> | no |
| <a name="input_source_connector_type"></a> [source\_connector\_type](#input\_source\_connector\_type) | Type of Source connector, such as Servicenow, S3, and so on | `string` | n/a | yes |
| <a name="input_tags"></a> [tags](#input\_tags) | Tag map for the resource | `map(string)` | `null` | no |
| <a name="input_tasks"></a> [tasks](#input\_tasks) | List of tasks to be performed in the flow. A Task that Amazon AppFlow performs while transferring the data in the flow run | <pre>list(object({<br>    source_fields      = list(string)          # Source fields to which a particular task is applied<br>    task_type          = string                # Particular task implementation that Amazon AppFlow performs. Valid values are Arithmetic, Filter, Map, Map_all, Mask, Merge, Passthrough, Truncate, and Validate<br>    destination_field  = optional(string)      # Field in a destination connector, or a field value against which Amazon AppFlow validates a source field<br>    task_properties    = optional(map(string)) # Map used to store task-related information. The execution service looks for particular information based on the TaskType. Valid keys are VALUE, VALUES, DATA_TYPE, UPPER_BOUND, LOWER_BOUND, SOURCE_DATA_TYPE, DESTINATION_DATA_TYPE, VALIDATION_ACTION, MASK_VALUE, MASK_LENGTH, TRUNCATE_LENGTH, MATH_OPERATION_FIELDS_ORDER, CONCAT_FORMAT, SUBFIELD_CATEGORY_MAP, and EXCLUDE_SOURCE_FIELDS_LIST<br>    connector_type     = string                # Type of the connector, e.g., "s3", "sapo_data"<br>    connector_operator = string                # Operation to be performed on the provided source fields   <br>  }))</pre> | n/a | yes |
| <a name="input_trigger_properties"></a> [trigger\_properties](#input\_trigger\_properties) | Configuration details of a schedule-triggered flow as defined by the user. only apply to the 'Scheduled' trigger type | <pre>object({<br>    schedule_expression  = string           # Scheduling expression that determines the rate at which the schedule will run, for example rate(5minutes)<br>    data_pull_mode       = optional(string) # Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are Incremental and Complete<br>    first_execution_from = optional(number) # Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp<br>    schedule_end_time    = optional(number) # Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp<br>    schedule_offset      = optional(number) # Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000<br>    schedule_start_time  = optional(number) # Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp<br>    timezone             = optional(string) # Time zone used when referring to the date and time of a scheduled-triggered flow, such as America/New_York<br>  })</pre> | `null` | no |
| <a name="input_trigger_type"></a> [trigger\_type](#input\_trigger\_type) | Flow Trigger type. Allowed: 'Scheduled', 'Event', and 'OnDemand' | `string` | `"OnDemand"` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_flow_arn"></a> [flow\_arn](#output\_flow\_arn) | Flow's ARN |
| <a name="output_flow_status"></a> [flow\_status](#output\_flow\_status) | The current status of the flow |
| <a name="output_flow_tags"></a> [flow\_tags](#output\_flow\_tags) | Map of tags assigned to the resource |
